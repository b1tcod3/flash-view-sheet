# SUBFASE 2.2: DISE√ëO DE ALGORITMOS
## Especificaci√≥n Detallada de Algoritmos para Separaci√≥n de Datos con Plantillas Excel

### üìã Objetivo
Definir algoritmos espec√≠ficos y optimizados para el procesamiento eficiente de separaci√≥n de datos, mapeo de columnas, generaci√≥n de nombres de archivos, y validaci√≥n con manejo de casos especiales.

### üßÆ Algoritmos Principales

#### **1. Algoritmo de Separaci√≥n Eficiente**

**Nombre**: `DataFrameSeparationAlgorithm`

**Prop√≥sito**: Separar DataFrame en grupos por columna espec√≠fica con optimizaci√≥n de memoria y rendimiento.

**Pseudoc√≥digo**:
```python
def separate_dataframe_efficiently(df: pd.DataFrame, separator_column: str, 
                                 enable_chunking: bool = True) -> Iterator[GroupData]:
    """
    Algoritmo principal de separaci√≥n con optimizaci√≥n de memoria
    
    Args:
        df: DataFrame a separar
        separator_column: Columna para agrupar
        enable_chunking: Habilitar chunking autom√°tico
    
    Yields:
        GroupData: Datos del grupo con metadata
    """
    
    # FASE 1: Validaci√≥n y Preparaci√≥n
    if separator_column not in df.columns:
        raise ValueError(f"Columna '{separator_column}' no existe en DataFrame")
    
    # Detectar valores nulos para manejo especial
    null_mask = df[separator_column].isnull()
    if null_mask.any():
        null_group_name = handle_null_values(separator_column)
        yield GroupData(name=null_group_name, 
                       data=df[null_mask], 
                       is_null_group=True)
    
    # FASE 2: Optimizaci√≥n para chunking
    if enable_chunking and should_use_chunking(df):
        for chunk in iterate_dataframe_in_chunks(df, chunk_size=get_optimal_chunk_size()):
            for group_data in process_chunk_separation(chunk, separator_column):
                yield group_data
    else:
        # FASE 3: Separaci√≥n directa para datasets peque√±os
        grouped_data = df[~null_mask].groupby(separator_column)
        total_groups = len(grouped_data)
        
        for group_name, group_df in grouped_data:
            yield GroupData(
                name=str(group_name), 
                data=group_df,
                group_size=len(group_df),
                group_index=get_group_index(group_name, grouped_data),
                total_groups=total_groups
            )
```

**Optimizaciones Espec√≠ficas**:
1. **Detecci√≥n Temprana de Nulos**: Procesar nulos por separado para evitar overhead
2. **Chunking Inteligente**: Usar chunks solo cuando sea necesario
3. **Index Caching**: Cache de √≠ndices para grupos frecuentes
4. **Memory Profiling**: Monitoreo continuo de uso de memoria

**Complejidad**:
- **Tiempo**: O(n log n) para grouping + O(n) para procesamiento
- **Espacio**: O(k * chunk_size) donde k es n√∫mero de grupos

#### **2. Algoritmo de Mapeo DataFrame ‚Üí Excel**

**Nombre**: `ColumnMappingAlgorithm`

**Prop√≥sito**: Mapear columnas DataFrame a coordenadas Excel (A, B, C...) con validaci√≥n y presets autom√°ticos.

**Pseudoc√≥digo**:
```python
def map_dataframe_to_excel_columns(df_columns: List[str], 
                                 excel_columns: List[str],
                                 mapping_strategy: str = "positional") -> ColumnMapping:
    """
    Algoritmo de mapeo flexible entre DataFrame y Excel
    
    Args:
        df_columns: Columnas del DataFrame
        excel_columns: Columnas disponibles en plantilla Excel
        mapping_strategy: "positional", "by_name", "preset", "manual"
    
    Returns:
        ColumnMapping: Mapeo completo con validaci√≥n
    """
    
    mapping = ColumnMapping()
    
    if mapping_strategy == "positional":
        # Mapeo 1:1 por posici√≥n
        mapping.positional = create_positional_mapping(df_columns, excel_columns)
        
    elif mapping_strategy == "by_name":
        # Mapeo por coincidencia de nombres (case-insensitive)
        mapping.name_based = create_name_based_mapping(df_columns, excel_columns)
        
    elif mapping_strategy == "preset":
        # Mapeo usando preset predefinido
        mapping.preset = apply_preset_mapping(df_columns)
        
    elif mapping_strategy == "manual":
        # Mapeo manual (requiere configuraci√≥n UI)
        mapping.manual = validate_manual_mapping(df_columns, excel_columns)
    
    # FASE 2: Validaci√≥n y Completado Autom√°tico
    validation_result = validate_mapping_completeness(mapping, df_columns)
    
    if not validation_result.is_complete:
        # Completar autom√°ticamente columnas faltantes
        missing_mappings = validation_result.missing_columns
        auto_filled = auto_complete_mapping(missing_mappings, excel_columns)
        mapping.update(auto_filled)
    
    # FASE 3: Optimizaci√≥n de Columnas
    optimized_mapping = optimize_column_layout(mapping)
    
    return optimized_mapping

def create_positional_mapping(df_columns: List[str], 
                            excel_columns: List[str]) -> Dict[str, str]:
    """Mapeo posicional 1:1"""
    mapping = {}
    min_columns = min(len(df_columns), len(excel_columns))
    
    for i in range(min_columns):
        df_col = df_columns[i]
        excel_col = excel_columns[i]
        mapping[df_col] = excel_col
    
    return mapping

def create_name_based_mapping(df_columns: List[str], 
                            excel_columns: List[str]) -> Dict[str, str]:
    """Mapeo por coincidencia de nombres (fuzzy matching)"""
    mapping = {}
    
    for df_col in df_columns:
        best_match = find_best_column_match(df_col, excel_columns)
        if best_match:
            mapping[df_col] = best_match
    
    return mapping

def auto_complete_mapping(missing_columns: List[str], 
                        available_excel_columns: List[str]) -> Dict[str, str]:
    """Completar autom√°ticamente columnas faltantes"""
    mapping = {}
    used_excel_cols = set()
    
    # Priorizar columnas por tipo de datos
    for col in missing_columns:
        best_excel_col = find_most_suitable_excel_column(
            col, available_excel_columns, used_excel_cols
        )
        if best_excel_col:
            mapping[col] = best_excel_col
            used_excel_cols.add(best_excel_col)
    
    return mapping
```

**Presets de Mapeo Predefinidos**:
```python
COLUMN_MAPPING_PRESETS = {
    "ventas_empresariales": {
        "fecha": "A", "region": "B", "vendedor": "C", "producto": "D",
        "cantidad": "E", "precio_unitario": "F", "total": "G", "comision": "H"
    },
    "reportes_financieros": {
        "periodo": "A", "categoria": "B", "subcategoria": "C", "ingresos": "D",
        "gastos": "E", "utilidad": "F", "margen": "G", "crecimiento": "H"
    },
    "datos_cientificos": {
        "experimento": "A", "muestra": "B", "parametro": "C", "valor": "D",
        "unidad": "E", "precision": "F", "fecha_medicion": "G", "observaciones": "H"
    }
}

def apply_preset_mapping(df_columns: List[str]) -> Dict[str, str]:
    """Aplicar preset m√°s compatible con columnas disponibles"""
    best_preset = None
    best_score = 0
    
    for preset_name, preset_mapping in COLUMN_MAPPING_PRESETS.items():
        score = calculate_preset_compatibility(df_columns, preset_mapping)
        if score > best_score:
            best_score = score
            best_preset = preset_mapping
    
    return best_preset if best_score > 0.7 else {}
```

#### **3. Algoritmo de Generaci√≥n de Nombres con Templates**

**Nombre**: `FileNamingTemplateProcessor`

**Prop√≥sito**: Procesar plantillas de nombres de archivo con placeholders din√°micos y validaci√≥n de compatibilidad del SO.

**Pseudoc√≥digo**:
```python
class FileNamingTemplateProcessor:
    PLACEHOLDER_PROCESSORS = {
        '{valor}': lambda group: sanitize_string(str(group['valor'])),
        '{columna}': lambda group: sanitize_string(str(group['columna'])),
        '{fecha}': lambda group: datetime.now().strftime('%Y-%m-%d'),
        '{fecha_hora}': lambda group: datetime.now().strftime('%Y-%m-%d_%H%M'),
        '{filas}': lambda group: str(group['filas']),
        '{indice}': lambda group: f"{group['indice']:02d}",  # 01, 02, 03...
        '{timestamp}': lambda group: str(int(time.time())),
        '{tama√±o_archivo}': lambda group: format_file_size(group['tama√±o_mb']),
        '{hash_md5}': lambda group: generate_short_hash(group['datos'])
    }
    
    def process_template(self, template: str, group_info: Dict) -> str:
        """
        Procesar plantilla de nombre de archivo
        
        Args:
            template: Plantilla con placeholders (ej: "Reporte_{valor}_{fecha}.xlsx")
            group_info: Informaci√≥n del grupo para substituir placeholders
        
        Returns:
            str: Nombre de archivo procesado y sanitizado
        """
        
        # FASE 1: Validaci√≥n de plantilla
        template_validation = self.validate_template_syntax(template)
        if not template_validation.is_valid:
            raise ValueError(f"Plantilla inv√°lida: {template_validation.errors}")
        
        # FASE 2: Extracci√≥n de placeholders
        placeholders = self.extract_placeholders(template)
        
        # FASE 3: Procesamiento de cada placeholder
        processed_template = template
        for placeholder in placeholders:
            if placeholder in self.PLACEHOLDER_PROCESSORS:
                try:
                    replacement = self.PLACEHOLDER_PROCESSORS[placeholder](group_info)
                    processed_template = processed_template.replace(placeholder, replacement)
                except Exception as e:
                    # Fallback para placeholder problem√°tico
                    fallback_value = self.get_fallback_value(placeholder)
                    processed_template = processed_template.replace(placeholder, fallback_value)
            else:
                # Placeholder desconocido - mantener original pero sanitizar
                sanitized_placeholder = sanitize_filename(placeholder)
                processed_template = processed_template.replace(placeholder, sanitized_placeholder)
        
        # FASE 4: Sanitizaci√≥n final para compatibilidad del SO
        sanitized_filename = self.sanitize_filename_for_os(processed_template)
        
        # FASE 5: Validaci√≥n de longitud y caracteres
        final_filename = self.validate_filename_constraints(sanitized_filename)
        
        return final_filename
    
    def extract_placeholders(self, template: str) -> List[str]:
        """Extraer todos los placeholders v√°lidos de la plantilla"""
        import re
        pattern = r'\{([a-zA-Z_][a-zA-Z0-9_]*)\}'
        matches = re.findall(pattern, template)
        return [f'{{{match}}}' for match in matches]
    
    def sanitize_filename_for_os(self, filename: str) -> str:
        """Sanitizar nombre para compatibilidad cross-platform"""
        # Caracteres prohibidos en diferentes SO
        forbidden_chars = r'[<>:"/\\|?*]'
        sanitized = re.sub(forbidden_chars, '_', filename)
        
        # Remover puntos m√∫ltiples al final
        sanitized = re.sub(r'\.+$', '', sanitized)
        
        # Asegurar que no est√© vac√≠o
        if not sanitized.strip():
            sanitized = 'archivo_sin_nombre'
        
        # Normalizar espacios
        sanitized = ' '.join(sanitized.split())
        
        return sanitized
    
    def validate_filename_constraints(self, filename: str) -> str:
        """Validar restricciones de longitud y formato"""
        MAX_LENGTH = 255  # Universal
        
        if len(filename) > MAX_LENGTH:
            # Truncar inteligentemente preservando extensi√≥n
            name_part, ext = os.path.splitext(filename)
            available_length = MAX_LENGTH - len(ext)
            truncated_name = name_part[:available_length-3] + '...'
            filename = truncated_name + ext
        
        return filename

def generate_unique_filename(base_filename: str, existing_files: Set[str]) -> str:
    """Generar nombre √∫nico para evitar conflictos"""
    if base_filename not in existing_files:
        return base_filename
    
    # Intentar con n√∫meros secuenciales
    name_part, ext = os.path.splitext(base_filename)
    counter = 1
    
    while True:
        new_filename = f"{name_part}_{counter:02d}{ext}"
        if new_filename not in existing_files:
            return new_filename
        counter += 1
        
        # L√≠mite de seguridad
        if counter > 999:
            # Usar timestamp como √∫ltimo recurso
            timestamp = str(int(time.time()))
            new_filename = f"{name_part}_{timestamp}{ext}"
            return new_filename
```

#### **4. Algoritmo de Validaci√≥n y Pre-an√°lisis**

**Nombre**: `DataValidationAlgorithm`

**Prop√≥sito**: Validar configuraci√≥n completa y analizar datos antes de proceder con separaci√≥n.

**Pseudoc√≥digo**:
```python
class DataValidationAlgorithm:
    def validate_complete_configuration(self, df: pd.DataFrame, 
                                      config: ExportSeparatedConfig) -> ValidationResult:
        """
        Validaci√≥n exhaustiva de configuraci√≥n y datos
        
        Returns:
            ValidationResult: Resultado completo con warnings y errores
        """
        result = ValidationResult()
        result.start_validation()
        
        # FASE 1: Validaci√≥n de datos de entrada
        data_validation = self.validate_input_data(df)
        result.merge(data_validation)
        
        # FASE 2: Validaci√≥n de configuraci√≥n
        config_validation = self.validate_configuration(config, df)
        result.merge(config_validation)
        
        # FASE 3: Validaci√≥n de plantilla Excel
        if config.template_path:
            template_validation = self.validate_excel_template(config.template_path)
            result.merge(template_validation)
        
        # FASE 4: An√°lisis predictivo
        analysis = self.predict_processing_requirements(df, config)
        result.analysis = analysis
        
        # FASE 5: Validaci√≥n de recursos del sistema
        system_validation = self.validate_system_resources(analysis)
        result.merge(system_validation)
        
        result.finish_validation()
        return result
    
    def validate_input_data(self, df: pd.DataFrame) -> ValidationResult:
        """Validar DataFrame de entrada"""
        result = ValidationResult()
        
        # Verificar DataFrame vac√≠o
        if df.empty:
            result.add_error("DataFrame est√° vac√≠o")
            return result
        
        # Verificar columnas
        if len(df.columns) == 0:
            result.add_error("DataFrame no tiene columnas")
            return result
        
        # Verificar nombres de columnas duplicados
        duplicated_cols = df.columns[df.columns.duplicated()].tolist()
        if duplicated_cols:
            result.add_warning(f"Columnas duplicadas detectadas: {duplicated_cols}")
        
        # Verificar tipos de datos problem√°ticos
        problematic_types = self.detect_problematic_data_types(df)
        if problematic_types:
            result.add_info(f"Tipos de datos problem√°ticos: {problematic_types}")
        
        # An√°lisis de memoria
        memory_usage_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_usage_mb > 2048:  # 2GB
            result.add_warning(f"Uso de memoria alto: {memory_usage_mb:.1f}MB")
        
        return result
    
    def predict_processing_requirements(self, df: pd.DataFrame, 
                                      config: ExportSeparatedConfig) -> ProcessingAnalysis:
        """An√°lisis predictivo de requisitos de procesamiento"""
        analysis = ProcessingAnalysis()
        
        # Estimaci√≥n de grupos
        if config.separator_column in df.columns:
            unique_values = df[config.separator_column].nunique()
            null_count = df[config.separator_column].isnull().sum()
            analysis.estimated_groups = unique_values + (1 if null_count > 0 else 0)
        else:
            analysis.estimated_groups = 0
        
        # Estimaci√≥n de tama√±o de archivos
        avg_rows_per_group = len(df) / max(analysis.estimated_groups, 1)
        analysis.estimated_rows_per_group = avg_rows_per_group
        
        # Predicci√≥n de tiempo de procesamiento
        processing_time_per_mb = 0.5  # Segundos por MB (estimado)
        total_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        analysis.estimated_processing_time = total_mb * processing_time_per_mb
        
        # Predicci√≥n de uso de memoria pico
        peak_memory_mb = total_mb * 1.5  # 50% overhead por procesamiento
        analysis.estimated_peak_memory_mb = peak_memory_mb
        
        # Recomendaciones de optimizaci√≥n
        recommendations = []
        if analysis.estimated_groups > 100:
            recommendations.append("Considerar usar chunking para mejor rendimiento")
        if analysis.estimated_processing_time > 300:
            recommendations.append("Procesamiento largo detectado - habilitar progreso detallado")
        if peak_memory_mb > 2048:
            recommendations.append("Alto uso de memoria - considerar procesamiento por chunks")
        
        analysis.recommendations = recommendations
        
        return analysis
```

### ‚ö° Algoritmos de Optimizaci√≥n

#### **5. Algoritmo de Chunking Inteligente**

**Nombre**: `IntelligentChunkingAlgorithm`

**Prop√≥sito**: Determinar estrategia √≥ptima de chunking basada en caracter√≠sticas del dataset.

```python
def determine_optimal_chunking_strategy(df: pd.DataFrame, 
                                      config: ExportSeparatedConfig) -> ChunkingStrategy:
    """
    Algoritmo para determinar estrategia √≥ptima de chunking
    
    Estrategia basada en:
    1. Tama√±o total del DataFrame
    2. N√∫mero de grupos √∫nicos
    3. Distribuci√≥n de tama√±os de grupo
    4. Recursos del sistema disponibles
    """
    
    total_rows = len(df)
    memory_usage_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
    unique_groups = df[config.separator_column].nunique() if config.separator_column in df.columns else 0
    
    # An√°lisis de distribuci√≥n de grupos
    if unique_groups > 0:
        group_sizes = df.groupby(config.separator_column).size()
        largest_group_size = group_sizes.max()
        smallest_group_size = group_sizes.min()
        group_size_variance = group_sizes.var()
    else:
        largest_group_size = total_rows
        smallest_group_size = total_rows
        group_size_variance = 0
    
    # Decisi√≥n de estrategia
    if memory_usage_mb > 2048 or total_rows > 1000000:
        # Estrategia agresiva para datasets muy grandes
        return ChunkingStrategy.AGGRESSIVE
        
    elif unique_groups > 100 and group_size_variance > 10000:
        # Muchos grupos con tama√±os variables
        return ChunkingStrategy.GROUP_BASED
        
    elif largest_group_size > 50000:
        # Algunos grupos muy grandes
        return ChunkingStrategy.SIZE_BASED
        
    elif memory_usage_mb > 500:
        # Dataset moderadamente grande
        return ChunkingStrategy.MODERATE
        
    else:
        # Dataset peque√±o - procesamiento directo
        return ChunkingStrategy.NONE

class ChunkingStrategy:
    NONE = "none"           # Procesamiento directo
    MODERATE = "moderate"   # Chunking conservador
    SIZE_BASED = "size"     # Basado en tama√±o de grupos
    GROUP_BASED = "group"   # Basado en n√∫mero de grupos
    AGGRESSIVE = "aggressive"  # Chunking agresivo
```

#### **6. Algoritmo de Preservaci√≥n de Formato Excel**

**Nombre**: `ExcelFormatPreservationAlgorithm`

**Prop√≥sito**: Insertar datos en plantilla Excel preservando 100% del formato original.

```python
def preserve_excel_format_during_insert(workbook: Workbook, 
                                       start_cell: str, 
                                       data_df: pd.DataFrame,
                                       column_mapping: Dict[str, str]) -> bool:
    """
    Algoritmo para preservar formato Excel durante inserci√≥n de datos
    
    Estrategia:
    1. Detectar y cachear formato existente
    2. Insertar datos sin alterar estilos
    3. Restaurar formato si es necesario
    4. Optimizar performance de openpyxl
    """
    
    # FASE 1: An√°lisis de formato existente
    sheet = workbook.active
    start_row, start_col = cell_coordinates_to_indices(start_cell)
    
    # Detectar √°rea de datos existente
    existing_data_range = detect_existing_data_range(sheet, start_cell)
    existing_formats = cache_existing_formats(sheet, existing_data_range)
    
    # FASE 2: Preparaci√≥n para inserci√≥n optimizada
    # Deshabilitar c√°lculo autom√°tico de f√≥rmulas
    sheet.sheet_properties.tabColor = sheet.sheet_properties.tabColor
    
    # FASE 3: Inserci√≥n de datos con formato preservado
    success = True
    for row_idx, (_, row_data) in enumerate(data_df.iterrows()):
        excel_row = start_row + row_idx
        
        for df_col, excel_col_letter in column_mapping.items():
            if df_col in data_df.columns:
                excel_col_idx = column_letter_to_index(excel_col_letter)
                cell = sheet.cell(row=excel_row, column=excel_col_idx)
                
                # Insertar valor preservando formato
                value = row_data[df_col]
                if pd.isna(value):
                    cell.value = None
                else:
                    cell.value = value
                
                # Preservar formato original si existe
                if (excel_row, excel_col_idx) in existing_formats:
                    cell.font = existing_formats[(excel_row, excel_col_idx)]['font']
                    cell.fill = existing_formats[(excel_row, excel_col_idx)]['fill']
                    cell.border = existing_formats[(excel_row, excel_col_idx)]['border']
                    cell.number_format = existing_formats[(excel_row, excel_col_idx)]['number_format']
    
    # FASE 4: Optimizaci√≥n final
    workbook._archive.close()  # Forzar escritura
    
    return success

def cache_existing_formats(sheet: Worksheet, data_range: str) -> Dict:
    """Cachear formatos existentes para preservaci√≥n"""
    formats = {}
    
    if not data_range:
        return formats
    
    try:
        for row in sheet[data_range]:
            for cell in row:
                if cell.value is not None:  # Solo cachear celdas con contenido
                    formats[(cell.row, cell.column)] = {
                        'font': copy_font(cell.font),
                        'fill': copy_fill(cell.fill),
                        'border': copy_border(cell.border),
                        'number_format': cell.number_format,
                        'alignment': copy_alignment(cell.alignment)
                    }
    except Exception as e:
        print(f"Warning: No se pudo cachear formato completo: {e}")
        # Fallback: cachear solo n√∫mero de formato
        for row in sheet[data_range]:
            for cell in row:
                if cell.value is not None:
                    formats[(cell.row, cell.column)] = {
                        'number_format': cell.number_format
                    }
    
    return formats
```

### üîÑ Algoritmos de Manejo de Errores

#### **7. Algoritmo de Recovery y Rollback**

**Nombre**: `ErrorRecoveryAlgorithm`

**Prop√≥sito**: Recovery autom√°tico en caso de fallos parciales con rollback de archivos inconsistentes.

```python
class ErrorRecoveryAlgorithm:
    def __init__(self, output_directory: str):
        self.output_directory = output_directory
        self.created_files = []
        self.backup_directory = None
    
    def execute_with_recovery(self, export_function: Callable) -> ExportResult:
        """Ejecutar exportaci√≥n con recovery autom√°tico"""
        
        # FASE 1: Preparaci√≥n del entorno
        self.setup_recovery_environment()
        
        try:
            # FASE 2: Ejecuci√≥n con monitoreo
            result = export_function()
            
            # FASE 3: Verificaci√≥n post-proceso
            if self.verify_export_completeness(result):
                self.cleanup_recovery_environment()
                return result
            else:
                # FASE 4: Recovery en caso de inconsistencia
                return self.perform_recovery(result)
                
        except Exception as e:
            # FASE 5: Recovery por excepci√≥n
            return self.handle_critical_error(e)
    
    def setup_recovery_environment(self):
        """Preparar entorno para recovery"""
        # Crear directorio de backup
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_directory = os.path.join(
            self.output_directory, f".backup_{timestamp}"
        )
        os.makedirs(self.backup_directory, exist_ok=True)
    
    def perform_recovery(self, partial_result: ExportResult) -> ExportResult:
        """Realizar recovery de exportaci√≥n parcial"""
        
        recovery_result = ExportResult()
        recovery_result.success = False
        recovery_result.recovery_attempted = True
        
        # Verificar integridad de archivos creados
        valid_files = []
        corrupted_files = []
        
        for file_path in partial_result.files_created:
            if self.verify_file_integrity(file_path):
                valid_files.append(file_path)
            else:
                corrupted_files.append(file_path)
                # Mover a backup para an√°lisis
                backup_path = os.path.join(self.backup_directory, os.path.basename(file_path))
                shutil.move(file_path, backup_path)
        
        recovery_result.files_created = valid_files
        recovery_result.files_corrupted = corrupted_files
        recovery_result.recovery_successful = len(valid_files) > 0
        
        # Limpiar archivos corruptos
        self.cleanup_corrupted_files(corrupted_files)
        
        return recovery_result
    
    def verify_file_integrity(self, file_path: str) -> bool:
        """Verificar integridad de archivo Excel"""
        try:
            # Intentar abrir con openpyxl
            workbook = openpyxl.load_workbook(file_path)
            workbook.close()
            return True
        except Exception:
            return False
```

### üìä M√©tricas de Rendimiento de Algoritmos

#### **An√°lisis de Complejidad**

| Algoritmo | Complejidad Temporal | Complejidad Espacial | Optimizaciones |
|-----------|---------------------|---------------------|----------------|
| `DataFrameSeparationAlgorithm` | O(n log n) | O(k √ó chunk_size) | Chunking inteligente |
| `ColumnMappingAlgorithm` | O(n √ó m) | O(n + m) | Presets autom√°ticos |
| `FileNamingTemplateProcessor` | O(t √ó p) | O(t) | Cache de resultados |
| `DataValidationAlgorithm` | O(n) | O(1) | An√°lisis incremental |
| `ExcelFormatPreservationAlgorithm` | O(n √ó c) | O(f) | Cache de formatos |
| `ErrorRecoveryAlgorithm` | O(f log f) | O(f) | Verificaci√≥n paralela |

#### **Benchmarks Estimados**

| Dataset Size | Rows | Groups | Processing Time | Memory Peak |
|-------------|------|--------|----------------|-------------|
| Small | < 10K | < 10 | < 30s | < 100MB |
| Medium | 10K-100K | 10-100 | < 3min | < 500MB |
| Large | 100K-1M | 100-1000 | < 15min | < 2GB |
| Very Large | 1M+ | 1000+ | < 1hr | < 4GB |

### ‚úÖ Criterios de Validaci√≥n de Algoritmos

#### **Testing de Algoritmos**
1. **Unit Tests**: Cada algoritmo debe tener > 95% cobertura
2. **Performance Tests**: Benchmarks contra datasets est√°ndar
3. **Edge Case Tests**: Valores nulos, caracteres especiales, l√≠mites del SO
4. **Integration Tests**: Algoritmos trabajando juntos
5. **Stress Tests**: Datasets extremos para identificar l√≠mites

#### **M√©tricas de Calidad**
- **Accuracy**: 100% preservaci√≥n de datos
- **Performance**: < 3x tiempo de exportaci√≥n normal
- **Reliability**: > 99% √©xito sin intervenci√≥n manual
- **Memory Efficiency**: < 2GB pico para datasets de 1M filas
- **Format Preservation**: 100% preservaci√≥n de formato Excel

---

**CONCLUSI√ìN**: Los algoritmos especificados proporcionan una base s√≥lida y optimizada para la separaci√≥n eficiente de datos con plantillas Excel, con √©nfasis en rendimiento, confiabilidad y preservaci√≥n de formato. Cada algoritmo incluye optimizaciones espec√≠ficas y manejo robusto de casos especiales.