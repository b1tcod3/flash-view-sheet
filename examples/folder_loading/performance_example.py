#!/usr/bin/env python3
"""
Ejemplo de rendimiento para carga de carpeta con Flash Sheet

Este ejemplo demuestra optimizaciones para procesar grandes volÃºmenes de datos:
- Procesamiento por lotes (chunked processing)
- Carga diferida de metadatos
- Monitoreo de progreso
- GestiÃ³n eficiente de memoria
"""

import sys
import os
import time
import pandas as pd

# AÃ±adir el directorio raÃ­z del proyecto al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from core.loaders.folder_loader import FolderLoader
from core.consolidation.excel_consolidator import ExcelConsolidator

def performance_comparison():
    """Comparar mÃ©todos de procesamiento para demostrar optimizaciones"""

    print("ğŸš€ Flash Sheet - Comparativa de Rendimiento")
    print("=" * 55)

    folder_path = os.path.join(os.path.dirname(__file__), 'sample_data')

    try:
        # MÃ©todo 1: Procesamiento tradicional (uno por uno)
        print("1ï¸âƒ£ MÃ©todo 1: Procesamiento tradicional")
        start_time = time.time()

        loader = FolderLoader(folder_path)
        consolidator1 = ExcelConsolidator()

        files = loader.get_excel_files()
        for file_path in files:
            df = pd.read_excel(file_path)
            consolidator1.add_dataframe(df, os.path.basename(file_path))

        result1 = consolidator1.consolidate()
        time1 = time.time() - start_time

        print(f"   â±ï¸  Tiempo: {time1:.2f} segundos")
        print(f"   ğŸ“Š Resultado: {len(result1)} filas, {len(result1.columns)} columnas")
        print()

        # MÃ©todo 2: Procesamiento por lotes optimizado
        print("2ï¸âƒ£ MÃ©todo 2: Procesamiento por lotes (Optimizado)")
        start_time = time.time()

        consolidator2 = ExcelConsolidator()

        # Callback para mostrar progreso
        def progress_callback(progress):
            print(f"     Progreso: {progress:.1f}%")

        result2 = consolidator2.consolidate_chunked(
            files,
            chunk_size=2,  # Procesar de 2 en 2 archivos
            progress_callback=progress_callback
        )
        time2 = time.time() - start_time

        print(f"   â±ï¸  Tiempo: {time2:.2f} segundos")
        print(f"   ğŸ“Š Resultado: {len(result2)} filas, {len(result2.columns)} columnas")
        print()

        # ComparaciÃ³n
        print("3ï¸âƒ£ ComparaciÃ³n de rendimiento:")
        speedup = time1 / time2 if time2 > 0 else 1
        print(f"   ğŸš€ AceleraciÃ³n: {speedup:.2f}x mÃ¡s rÃ¡pido")
        print(f"   ğŸ’¾ Ahorro de tiempo: {(time1 - time2):.1f} segundos")
        print()

        # Verificar que los resultados son equivalentes
        print("4ï¸âƒ£ VerificaciÃ³n de integridad:")
        if len(result1) == len(result2) and len(result1.columns) == len(result2.columns):
            print("   âœ… Resultados equivalentes")
        else:
            print("   âŒ Resultados diferentes - posible error")

        # Comparar algunas estadÃ­sticas
        if 'Total' in result1.columns and 'Total' in result2.columns:
            sum1 = result1['Total'].sum()
            sum2 = result2['Total'].sum()
            if abs(sum1 - sum2) < 0.01:
                print(f"   âœ… Suma total correcta: ${sum1:.2f}")
            else:
                print(f"   âŒ Diferencia en suma: ${abs(sum1 - sum2):.2f}")
        print()

    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n: {e}")
        import traceback
        traceback.print_exc()

def large_scale_simulation():
    """Simular procesamiento de un gran nÃºmero de archivos"""

    print("5ï¸âƒ£ SimulaciÃ³n de procesamiento a gran escala")
    print("=" * 50)

    # Crear archivos adicionales para simular carga mayor
    import tempfile
    import shutil

    temp_dir = tempfile.mkdtemp()
    try:
        # Copiar archivos existentes y crear mÃ¡s
        sample_dir = os.path.join(os.path.dirname(__file__), 'sample_data')

        # Copiar archivos existentes
        for filename in os.listdir(sample_dir):
            if filename.endswith('.xlsx'):
                shutil.copy2(
                    os.path.join(sample_dir, filename),
                    os.path.join(temp_dir, filename)
                )

        # Crear archivos adicionales (simulando mÃ¡s datos)
        base_df = pd.read_excel(os.path.join(sample_dir, 'ventas_q1.xlsx'))

        for i in range(6, 16):  # Crear archivos ventas_q6.xlsx hasta ventas_q15.xlsx
            # Modificar ligeramente los datos para simular diferentes perÃ­odos
            modified_df = base_df.copy()
            modified_df['Total'] = modified_df['Total'] * (0.8 + i * 0.05)  # VariaciÃ³n en ventas

            filename = f'ventas_q{i}.xlsx'
            modified_df.to_excel(os.path.join(temp_dir, filename), index=False)

        print(f"   ğŸ“ Creados {len(os.listdir(temp_dir))} archivos de simulaciÃ³n en {temp_dir}")

        # Procesar con mÃ©todo optimizado
        print("   âš¡ Procesando con mÃ©todo optimizado...")
        start_time = time.time()

        loader = FolderLoader(temp_dir)
        consolidator = ExcelConsolidator()

        files = loader.get_excel_files()
        print(f"   ğŸ“Š Procesando {len(files)} archivos...")

        processed_count = 0
        def progress_callback(progress):
            nonlocal processed_count
            current_count = int(progress * len(files) / 100)
            if current_count > processed_count:
                print(f"     Progreso: {current_count}/{len(files)} archivos ({progress:.1f}%)")
                processed_count = current_count

        result = consolidator.consolidate_chunked(
            files,
            chunk_size=3,  # Procesar de 3 en 3 para simular optimizaciÃ³n
            progress_callback=progress_callback
        )

        processing_time = time.time() - start_time

        print(f"   â±ï¸  Tiempo de procesamiento: {processing_time:.2f} segundos")
        print(f"   ğŸ“Š Dataset final: {len(result)} filas, {len(result.columns)} columnas")
        print(f"   ğŸ“ˆ Rendimiento: {len(result) / processing_time:.1f} filas/segundo")
        print()

    except Exception as e:
        print(f"âŒ Error en simulaciÃ³n: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Limpiar archivos temporales
        shutil.rmtree(temp_dir, ignore_errors=True)

def memory_optimization_demo():
    """Demostrar optimizaciones de memoria"""

    print("6ï¸âƒ£ Optimizaciones de memoria")
    print("=" * 35)

    folder_path = os.path.join(os.path.dirname(__file__), 'sample_data')
    loader = FolderLoader(folder_path)

    print("   ğŸ“Š Demostrando cachÃ© de metadatos:")

    # Primera carga (sin cachÃ©)
    start_time = time.time()
    meta1 = loader.get_file_metadata(loader.get_excel_files()[0])
    time1 = time.time() - start_time

    # Segunda carga (con cachÃ©)
    start_time = time.time()
    meta2 = loader.get_file_metadata(loader.get_excel_files()[0])
    time2 = time.time() - start_time

    print(f"   ğŸ“Š Primera carga (sin cachÃ©): {time1:.4f} segundos")
    print(f"   ğŸ“Š Segunda carga (con cachÃ©): {time2:.4f} segundos")
    print(f"   ğŸš€ AceleraciÃ³n con cachÃ©: {time1 / time2:.1f}x mÃ¡s rÃ¡pido")
    print()

    # Limpiar cachÃ© para demostrar
    print("   ğŸ§¹ Limpiando cachÃ©...")
    loader.clear_metadata_cache()

    start_time = time.time()
    meta3 = loader.get_file_metadata(loader.get_excel_files()[0])
    time3 = time.time() - start_time

    print(f"   ğŸ“Š DespuÃ©s de limpiar cachÃ©: {time3:.4f} segundos")
    print()

def main():
    """FunciÃ³n principal del ejemplo de rendimiento"""

    print("ğŸ¯ Flash Sheet - Ejemplo de Optimizaciones de Rendimiento")
    print("=" * 65)
    print()

    try:
        performance_comparison()
        large_scale_simulation()
        memory_optimization_demo()

        print("âœ… Â¡Todos los ejemplos de rendimiento completados!")
        print()
        print("ğŸ’¡ Lecciones aprendidas:")
        print("   â€¢ El procesamiento por lotes reduce el uso de memoria")
        print("   â€¢ El cachÃ© de metadatos acelera operaciones repetidas")
        print("   â€¢ Los callbacks de progreso mejoran la experiencia del usuario")
        print("   â€¢ Para datasets grandes, usa consolidate_chunked()")

    except Exception as e:
        print(f"âŒ Error general: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()